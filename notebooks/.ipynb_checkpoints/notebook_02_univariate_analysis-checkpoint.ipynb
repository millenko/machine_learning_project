{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a73dbcc-7251-47ac-b0e0-187ef6cf76c7",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26596539-3bf0-4c24-99e8-61689b2809eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.read_csv(\"../data/02.csv\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e31b9-047c-4aae-b93e-af5d15e3f0c5",
   "metadata": {},
   "source": [
    "Before engaging in deeper analysis, let's split the variables into categories and quantitative variables.\\\n",
    "The split is based on the number of unique values per variable, and nature of the variable.\\\n",
    "The threshold that comes up is 5:\n",
    "- those with less than 5 unique values, the variables appear to be categorical\n",
    "- 5 unique values and more, all the variables appear to be quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc41150-650d-41b0-88be-9a7c024bb8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3363a-a4e6-44e7-9da4-f1903ee2b06a",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100a1ff-4de8-4395-9387-1c77a4e39b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat = df[df.columns[df.nunique() < 5]]\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4938b36-8349-4e53-a882-13e4ada9cdb6",
   "metadata": {},
   "source": [
    "Of **119206 bookings**:\n",
    "- **37% were canceled**\n",
    "- **3% repeated guests**\n",
    "- **86% agent** bookings, 6% company bookings\n",
    "- **66% city hotel** bookings, 34% resort bookings\n",
    "- meals: **77% bed&breakfast**, 12% half-breakfast, 10% no meal, 1% full breakfast\n",
    "- market segment: **47% online travel agent (TA)**, 20% offline TA, 17% groups, 11% direct, 4% corporate, 1% other segments\n",
    "- distribution channel: **82% TA**, 12% direct, 5% corporate\n",
    "- **72% reserved rooms were type A**, 16% D, 5% E, 7% other 6 types\n",
    "- **88% booked without deposit**, 12% with a deposit\n",
    "- **75% individual (transient) bookings**, 21% transient and associated to another transient booking, 4% other arrangements.\n",
    "\n",
    "Let's visualise them with countplots to double-check the findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb1c91-5f02-442b-a774-3822e4dbb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_tables(cat):\n",
    "    \"\"\"\n",
    "    Calculates the proportion of values for each categorical column in the DataFrame and\n",
    "    formats it into a final DataFrame with percentages and labeled columns.\n",
    "\n",
    "    Parameters:\n",
    "    cat (DataFrame): The DataFrame containing the categorical data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with one row per variable and columns for the proportions of\n",
    "               each category, labeled 'no' and 'yes'.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store each column's proportion table DataFrame\n",
    "    proportion_tables_list = []\n",
    "\n",
    "    # Calculate proportions for each column\n",
    "    for col in cat.columns:\n",
    "        # Get the proportion table for the column as a DataFrame\n",
    "        proportion_table = cat[col].value_counts(normalize=True).rename_axis(\"Category\").reset_index(name=\"Proportion\")\n",
    "        \n",
    "        # Add a column with the variable name\n",
    "        proportion_table[\"Variable\"] = col\n",
    "        \n",
    "        # Append this table to the list\n",
    "        proportion_tables_list.append(proportion_table)\n",
    "\n",
    "    # Concatenate all the proportion table DataFrames into one\n",
    "    proportion_tables_df = pd.concat(proportion_tables_list, ignore_index=True)\n",
    "\n",
    "    # Pivot the table to have one row per variable and columns for percentages of \"0\" and \"1\"\n",
    "    proportion_tables_df_pivoted = proportion_tables_df.pivot(index=\"Variable\", columns=\"Category\", values=\"Proportion\")\n",
    "\n",
    "    # Multiply by 100 and format as a string with percentage sign\n",
    "    formatted_percentage = proportion_tables_df_pivoted * 100\n",
    "    formatted_percentage = formatted_percentage.map(\"{:.2f}%\".format)\n",
    "\n",
    "    # Rename the columns to 'no' and 'yes' and reset the index\n",
    "    formatted_percentage.columns = [\"no\", \"yes\"]\n",
    "    proportion_tables_df_final = formatted_percentage.reset_index()\n",
    "\n",
    "    return proportion_tables_df_final\n",
    "\n",
    "# Use:\n",
    "proportion_tables_df_final = proportion_tables(cat)\n",
    "proportion_tables_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0424ec-ee7a-4211-8f32-653e7d7656d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_return_categorical_counts(cat, df):\n",
    "    \"\"\"\n",
    "    Creates count plots for each categorical column in the DataFrame and returns a DataFrame containing the count data.\n",
    "    \n",
    "    Parameters:\n",
    "    cat (DataFrame): The DataFrame containing the categorical columns.\n",
    "    df (DataFrame): The main DataFrame containing all data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing count data for each categorical column.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store count data DataFrames\n",
    "    count_data_list = []\n",
    "    \n",
    "    for column in cat:\n",
    "        plt.figure(figsize=(3, 5))\n",
    "        countplot = sns.countplot(\n",
    "            x=column, \n",
    "            hue=column, \n",
    "            data=df, \n",
    "            palette=\"Set1\"\n",
    "        )\n",
    "\n",
    "        # Optional customization\n",
    "        countplot.set_xlabel(\"Category Value\")\n",
    "        countplot.set_ylabel(\"Frequency\")\n",
    "        plt.legend(title=column, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "        # Prepare data for return\n",
    "        count_data = df[column].value_counts().rename_axis(\"Category\").reset_index(name=\"Counts\")\n",
    "        count_data[\"Variable\"] = column\n",
    "\n",
    "        # Annotate each bar with the height (count value)\n",
    "        for p in countplot.patches:\n",
    "            count = int(p.get_height())  # Get the bar count as integer\n",
    "            if count > 0:  # Only annotate bars with a count greater than zero\n",
    "                countplot.annotate(\n",
    "                    str(count), \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha=\"center\", va=\"center\", \n",
    "                    xytext=(0, 10),  # Offset the text by 10 points vertically to put it above the bar\n",
    "                    textcoords=\"offset points\"\n",
    "                )\n",
    "\n",
    "        # Set the y-limit of the plot to make space for the annotations\n",
    "        plt.ylim(0, max(df[column].value_counts()) * 1.1)  # Set y-limit to 10% above the highest bar\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        # Store count data DataFrame in the list\n",
    "        count_data_list.append(count_data)\n",
    "    \n",
    "    # Concatenate all count data into one DataFrame\n",
    "    all_count_data = pd.concat(count_data_list, ignore_index=True)\n",
    "    \n",
    "    return all_count_data\n",
    "\n",
    "# Use:\n",
    "all_counts = plot_and_return_categorical_counts(cat, df)\n",
    "all_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90a49e-9c0c-4a12-af02-274900991b16",
   "metadata": {},
   "source": [
    "# Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e088a7-ae48-4f20-9f9f-61fe9addf466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = df[df.columns[df.nunique() >= 5]]\n",
    "num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b75ba-15b8-48cf-8558-e1c5a3df496a",
   "metadata": {},
   "source": [
    "Of **119206 bookings**:\n",
    "- rooms were **booked 2 (median) - 3,5 (mean) months ahead**\n",
    "  - median < mean = more people tend to book closer to the stay-date\n",
    "  - but some far ahead bookings (2 years) drag the distribution tail to the right\n",
    "- **peak arrivals** are **mid-june to mid-july**, showing by the:\n",
    "  - arrival_date_week_number: 27 (mean) - 28 (median),\n",
    "  - arrival_date_day_of_month: 15 \n",
    "  - arrival_date_month: 6.6 (mean) - 7 (median)\n",
    "- bookings include at least **1 weekend day and/or 2 weekdays**\n",
    "- bookings were made for **2 adults**, no children or babies\n",
    "- **no history of cancelations**, which aligns with 97% being first-timers\n",
    "- **car parking space not required**\n",
    "- **no special requests**\n",
    "- **daily rate: 95 (median) - 102 (mean)**\n",
    "\n",
    "**Outliers:**\n",
    "- 2 years lead time\n",
    "- 19 weekends\n",
    "- 50 days\n",
    "- 55 adults\n",
    "- 10 children\n",
    "- 10 babies\n",
    "- 26 cancelations\n",
    "- 8 car parking spaces\n",
    "- 5 special requests\n",
    "-daily rate: -6.38\n",
    "- daily rate: 5400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a8119-e621-48ee-be44-253e15e814a8",
   "metadata": {},
   "source": [
    "## Normality\n",
    "### Visual check for normality\n",
    "The histograms of 11/14 variables show non-normal distribution, right-skewed, with most of the data concentrated on the lower end.\n",
    "- **arrival_date_week_number**: looks more uniform, but still not normal\n",
    "- **arrival_date_day_of_month**: fairly uniform\n",
    "- **arrival_date_month**: roughly symmetrical, but not normal or uniform distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93771c8f-1282-4106-8cd6-9d24bfe01fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.hist(figsize=(15, 20), bins=60, xlabelsize=10, ylabelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5509d-8d1d-4d54-95ac-3cc7406613f7",
   "metadata": {},
   "source": [
    "## Normality tests\n",
    "- The numerical variables have been the subject to two normality test, starting with the null hypothesis:\n",
    "  - H0: The sample is derived from a population that follows a normal distribution.\n",
    "  - at the significance level (alpha) is set at 0.05.\n",
    "\n",
    " \n",
    "- All the p-values from the **Shapiro-Wilk** test are very low, indicating a rejection of the null hypothesis of normality for every variable.\n",
    "- Likewise, the **Kolmogorov-Smirnov** test results also show p-values of close to zero, leading to the same conclusion.\\\n",
    "With these consistent results from multiple normality tests, along with the high assymetry and and long tails, the evidence strongly suggests that the **data is not normally distributed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fb50b-c941-4730-9ae4-1cd148325653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normality(num, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Tests normality for each numerical column in the DataFrame using the Shapiro-Wilk and \n",
    "    Kolmogorov-Smirnov tests, and returns the results in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    num (DataFrame): DataFrame containing numerical columns to test.\n",
    "    alpha (float): Significance level used to determine normality.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with the results of the normality tests.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store the results\n",
    "    columns = []\n",
    "    shapiro_statistics = []\n",
    "    shapiro_p_values = []\n",
    "    shapiro_diagnosis = []\n",
    "    ks_statistics = []\n",
    "    ks_p_values = []\n",
    "    ks_diagnosis = []\n",
    "\n",
    "    # Define a function to determine the diagnosis based on the p-value\n",
    "    def normality_diagnosis(p_value):\n",
    "        return \"Normal\" if p_value > alpha else \"Not normal\"\n",
    "\n",
    "    # Testing normality for each numerical column\n",
    "    for col in num.columns:\n",
    "        data = num[col].dropna()  # Exclude missing values from the test\n",
    "\n",
    "        # Shapiro-Wilk test with sample size check\n",
    "        if len(data) > 5000:\n",
    "            data_shapiro = data.sample(5000, random_state=1)\n",
    "        else:\n",
    "            data_shapiro = data\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(data_shapiro)\n",
    "        shapiro_statistics.append(shapiro_stat)\n",
    "        shapiro_p_values.append(shapiro_p)\n",
    "        shapiro_diagnosis.append(normality_diagnosis(shapiro_p))\n",
    "        \n",
    "        # Kolmogorov-Smirnov test with standardization\n",
    "        data_ks = (data - data.mean()) / data.std(ddof=0)\n",
    "        ks_stat, ks_p = stats.kstest(data_ks, 'norm')\n",
    "        ks_statistics.append(ks_stat)\n",
    "        ks_p_values.append(ks_p)\n",
    "        ks_diagnosis.append(normality_diagnosis(ks_p))\n",
    "        \n",
    "        columns.append(col)\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    normality_test_df = pd.DataFrame({\n",
    "        \"Column\": columns,\n",
    "        \"Shapiro Statistic\": shapiro_statistics,\n",
    "        \"Shapiro P-value\": shapiro_p_values,\n",
    "        \"Shapiro Diagnosis\": shapiro_diagnosis,\n",
    "        \"KS Statistic\": ks_statistics,\n",
    "        \"KS P-value\": ks_p_values,\n",
    "        \"KS Diagnosis\": ks_diagnosis\n",
    "    })\n",
    "\n",
    "    return normality_test_df\n",
    "\n",
    "# Use:\n",
    "normality_test_results = test_normality(num)\n",
    "display(normality_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd806301-c480-4312-b794-c188e211ab4d",
   "metadata": {},
   "source": [
    "## Shape of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c74682-a7da-47a5-88d8-6bd1fd4c7837",
   "metadata": {},
   "source": [
    "- **Skewness**:\n",
    "  - Absolute skewness value [0,2] indicates a distribution with a moderately asymmetric tail for 6/14 columns.\n",
    "  - **8/14 of the columns exhibit highly assymetric data.**\n",
    "  - \n",
    "- **Kurtosis**:\n",
    "  - A kurtosis value of around 3 is expected for a normal distribution. **None of the columns have normal distribution**.\n",
    "  - **9/14 columns have heavy tails or outliers** (high kurtosis >3).\n",
    "  - Low kurtosis (<3) suggests light tails or lack of outliers, which is case for 5/14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40273a-038e-4b81-9318-c21fad718fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_skewness_kurtosis(df, num_columns):\n",
    "    \"\"\"\n",
    "    Calculates skewness and kurtosis for each numerical column in the DataFrame and provides\n",
    "    a diagnosis based on their values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    num_columns (list): List of numerical column names to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with skewness, kurtosis, and their respective diagnoses for each column.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store the results and diagnoses\n",
    "    skewness = []\n",
    "    kurtosis = []\n",
    "    skew_diagnosis = []\n",
    "    kurt_diagnosis = []\n",
    "    columns = []\n",
    "\n",
    "    # Loop through the columns and calculate skew and kurtosis\n",
    "    for col in num_columns:\n",
    "        skew = df[col].skew()\n",
    "        kurt = df[col].kurt()\n",
    "        columns.append(col)\n",
    "        skewness.append(skew)\n",
    "        kurtosis.append(kurt)\n",
    "        \n",
    "        # Determine skew diagnosis\n",
    "        if abs(skew) > 2:\n",
    "            skew_diagnosis.append(\"Highly skewed\")\n",
    "        else:\n",
    "            skew_diagnosis.append(\"Moderately skewed\")\n",
    "\n",
    "        # Determine kurtosis diagnosis\n",
    "        if kurt > 3:\n",
    "            kurt_diagnosis.append(\"Heavy-tailed\")\n",
    "        elif kurt < 3:\n",
    "            kurt_diagnosis.append(\"Light-tailed\")\n",
    "        else:\n",
    "            kurt_diagnosis.append(\"Mesokurtic\")\n",
    "\n",
    "    # Create a DataFrame with the results and diagnoses\n",
    "    skew_kurt_diagnosis_df = pd.DataFrame({\n",
    "        \"Column\": columns,\n",
    "        \"Skewness\": skewness,\n",
    "        \"Kurtosis\": kurtosis,\n",
    "        \"Skew Diagnosis\": skew_diagnosis,\n",
    "        \"Kurtosis Diagnosis\": kurt_diagnosis\n",
    "    })\n",
    "\n",
    "    return skew_kurt_diagnosis_df\n",
    "\n",
    "# Use:\n",
    "skew_kurt_results = analyze_skewness_kurtosis(df, num.columns)\n",
    "skew_kurt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fab359-577d-4c3b-8060-630e4f889f37",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- univariate analysis:\n",
    "  - numerical variables:\n",
    "    - **normality, visual check** (histogram): 11/14 variables show **non-normal distribution**, right-skewed, most data concentrated on the lower end\n",
    "    - **normality test** (shapiro-wilk, kolmogorov-smirnov): **non-normal distribution** (all the p-values < 0.05)\n",
    "    - **shape of distribution**:\n",
    "      - skewness: 8/14 variables exhibit **highly assymetric data**, 6/14 moderately assymetric\n",
    "      - kurtosis: 9/14 variables have **heavy tails or outliers**, 5/14 light tails or lack of outliers\n",
    "---\n",
    "\n",
    "Next: notebook_03_bivariate_analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
