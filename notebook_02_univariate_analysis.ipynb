{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a73dbcc-7251-47ac-b0e0-187ef6cf76c7",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26596539-3bf0-4c24-99e8-61689b2809eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.read_csv(\"02.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e8ea3-f6f5-4bff-8f20-2135a9c43713",
   "metadata": {},
   "source": [
    "# Categories vs quantitative variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e31b9-047c-4aae-b93e-af5d15e3f0c5",
   "metadata": {},
   "source": [
    "Before engaging in deeper analysis, let's split the variables into categories and quantitative variables.\\\n",
    "The split is based on the number of unique values per variable, and nature of the variable.\\\n",
    "The threshold that comes up is 5:\n",
    "- those with less than 5 unique values, the variables appear to be categorical\n",
    "- 5 unique values and more, all the variables appear to be quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc41150-650d-41b0-88be-9a7c024bb8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3363a-a4e6-44e7-9da4-f1903ee2b06a",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100a1ff-4de8-4395-9387-1c77a4e39b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat = df[df.columns[df.nunique() < 5]]\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a84ed2-767c-43de-9c72-805091efe09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store each column's proportion table DataFrame\n",
    "proportion_tables_list = []\n",
    "\n",
    "for col in cat.columns:\n",
    "    # Get the proportion table for the column as a DataFrame\n",
    "    proportion_table = df[col].value_counts(normalize=True).rename_axis('Category').reset_index(name='Proportion')\n",
    "    \n",
    "    # Add a column with the variable name\n",
    "    proportion_table['Variable'] = col\n",
    "    \n",
    "    # Append this table to the list\n",
    "    proportion_tables_list.append(proportion_table)\n",
    "\n",
    "# Concatenate all the proportion table DataFrames into one\n",
    "proportion_tables_df = pd.concat(proportion_tables_list, ignore_index=True)\n",
    "\n",
    "# Pivot the table to have one row per variable and columns for percentages of \"0\" and \"1\"\n",
    "proportion_tables_df_pivoted = proportion_tables_df.pivot(index='Variable', columns='Category', values='Proportion')\n",
    "\n",
    "# Multiply by 100 and format as a string with percentage sign\n",
    "formatted_percentage = proportion_tables_df_pivoted * 100\n",
    "formatted_percentage = formatted_percentage.map(\"{:.2f}%\".format)\n",
    "\n",
    "# Rename the columns to 'no' and 'yes' and reset the index\n",
    "formatted_percentage.columns = ['no', 'yes']\n",
    "proportion_tables_df_final = formatted_percentage.reset_index()\n",
    "\n",
    "# Now 'proportion_tables_df_final' is a DataFrame with one row per variable and columns 'no' and 'yes' for percentages\n",
    "proportion_tables_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4938b36-8349-4e53-a882-13e4ada9cdb6",
   "metadata": {},
   "source": [
    "Of **119206 bookings**:\n",
    "- **37% were canceled**\n",
    "- **3% repeated guests**\n",
    "- **86% agent** bookings, 6% company bookings\n",
    "- **66% city hotel** bookings, 34% resort bookings\n",
    "- meals: **77% bed&breakfast**, 12% half-breakfast, 10% no meal, 1% full breakfast\n",
    "- market segment: **47% online travel agent (TA)**, 20% offline TA, 17% groups, 11% direct, 4% corporate, 1% other segments\n",
    "- distribution channel: **82% TA**, 12% direct, 5% corporate\n",
    "- **72% reserved rooms were type A**, 16% D, 5% E, 7% other 6 types\n",
    "- **88% booked without deposit**, 12% with a deposit\n",
    "- **75% individual (transient) bookings**, 21% transient and associated to another transient booking, 4% other arrangements.\n",
    "\n",
    "Let's visualise them with countplots to double-check the findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280527eb-d1c6-49a7-bf42-6e495ddb830d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in cat:\n",
    "    plt.figure(figsize=(3, 5))\n",
    "    countplot = sns.countplot(\n",
    "        x=column, \n",
    "        hue=column, \n",
    "        data=df, \n",
    "        palette=\"Set1\")\n",
    "\n",
    "    # Optional customization\n",
    "    countplot.set_xlabel(\"Category Value\")\n",
    "    countplot.set_ylabel(\"Frequency\")\n",
    "    plt.legend(title=column, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Annotate each bar with the height (count value)\n",
    "    for p in countplot.patches:\n",
    "        count = int(p.get_height())  # Get the bar count as integer\n",
    "        if count > 0:  # Only annotate bars with a count greater than zero\n",
    "            countplot.annotate(\n",
    "                str(count), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha=\"center\", va=\"center\", \n",
    "                xytext=(0, 10),  # Offset the text by 10 points vertically to put it above the bar\n",
    "                textcoords=\"offset points\"\n",
    "            )\n",
    "\n",
    "    # Set the y-limit of the plot to make space for the annotations\n",
    "    plt.ylim(0, max(df[column].value_counts()) * 1.1)  # Set y-limit to 10% above the highest bar\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90a49e-9c0c-4a12-af02-274900991b16",
   "metadata": {},
   "source": [
    "# Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e088a7-ae48-4f20-9f9f-61fe9addf466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = df[df.columns[df.nunique() >= 5]]\n",
    "num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b75ba-15b8-48cf-8558-e1c5a3df496a",
   "metadata": {},
   "source": [
    "Of **119206 bookings**:\n",
    "- rooms were **booked 2 (median) - 3,5 (mean) months ahead**\n",
    "  - median < mean = more people tend to book closer to the stay-date\n",
    "  - but some far ahead bookings (2 years) drag the distribution tail to the right\n",
    "- **peak arrivals** are **mid-june to mid-july**, showing by the:\n",
    "  - arrival_date_week_number: 27 (mean) - 28 (median),\n",
    "  - arrival_date_day_of_month: 15 \n",
    "  - arrival_date_month: 6.6 (mean) - 7 (median)\n",
    "- bookings include at least **1 weekend day and/or 2 weekdays**\n",
    "- bookings were made for **2 adults**, no children or babies\n",
    "- **no history of cancelations**, which aligns with 97% being first-timers\n",
    "- **car parking space not required**\n",
    "- **no special requests**\n",
    "- **daily rate: 95 (median) - 102 (mean)**\n",
    "\n",
    "**Outliers:**\n",
    "- 2 years lead time\n",
    "- 19 weekends\n",
    "- 50 days\n",
    "- 55 adults\n",
    "- 10 children\n",
    "- 10 babies\n",
    "- 26 cancelations\n",
    "- 8 car parking spaces\n",
    "- 5 special requests\n",
    "-daily rate: -6.38\n",
    "- daily rate: 5400\n",
    "\n",
    "Let's visualise them with countplots to confirm the findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a8119-e621-48ee-be44-253e15e814a8",
   "metadata": {},
   "source": [
    "# Visual check for normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93771c8f-1282-4106-8cd6-9d24bfe01fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.hist(figsize=(15, 20), bins=60, xlabelsize=10, ylabelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8168a5-2f44-433c-b018-0588c9420c43",
   "metadata": {},
   "source": [
    "The histograms of 11/14 variables show non-normal distribution, right-skewed, with most of the data concentrated on the lower end.\n",
    "- **arrival_date_week_number**: looks more uniform, but still not normal\n",
    "- **arrival_date_day_of_month**: fairly uniform\n",
    "- **arrival_date_month**: roughly symmetrical, but not normal or uniform distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd806301-c480-4312-b794-c188e211ab4d",
   "metadata": {},
   "source": [
    "# Shape of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfd277-58ca-4316-9d43-1e29754f5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the results and diagnoses\n",
    "skewness = []\n",
    "kurtosis = []\n",
    "skew_diagnosis = []\n",
    "kurt_diagnosis = []\n",
    "columns = []\n",
    "\n",
    "# Loop through the columns and calculate skew and kurtosis\n",
    "for col in num:\n",
    "    skew = df[col].skew()\n",
    "    kurt = df[col].kurt()\n",
    "    columns.append(col)\n",
    "    skewness.append(skew)\n",
    "    kurtosis.append(kurt)\n",
    "    \n",
    "    # Determine skew diagnosis\n",
    "    if abs(skew) > 2:\n",
    "        skew_diagnosis.append('Highly skewed')\n",
    "    else:\n",
    "        skew_diagnosis.append('Moderately skewed')\n",
    "\n",
    "    # Determine kurtosis diagnosis\n",
    "    if kurt > 3:\n",
    "        kurt_diagnosis.append('Heavy-tailed')\n",
    "    elif kurt < 3:\n",
    "        kurt_diagnosis.append('Light-tailed')\n",
    "    else:\n",
    "        kurt_diagnosis.append('Mesokurtic')\n",
    "\n",
    "# Create a DataFrame with the results and diagnoses\n",
    "skew_kurt_diagnosis_df = pd.DataFrame({\n",
    "    'Column': columns,\n",
    "    'Skewness': skewness,\n",
    "    'Kurtosis': kurtosis,\n",
    "    'Skew Diagnosis': skew_diagnosis,\n",
    "    'Kurtosis Diagnosis': kurt_diagnosis\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "skew_kurt_diagnosis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c74682-a7da-47a5-88d8-6bd1fd4c7837",
   "metadata": {},
   "source": [
    "- **Skewness**:\n",
    "  - Absolute skewness value [0,2] indicates a distribution with a moderately asymmetric tail for 6/14 columns.\n",
    "  - **8/14 of the columns exhibit highly assymetric data.**\n",
    "  - \n",
    "- **Kurtosis**:\n",
    "  - A kurtosis value of around 3 is expected for a normal distribution. **None of the columns have normal distribution**.\n",
    "  - **9/14 columns have heavy tails or outliers** (high kurtosis >3).\n",
    "  - Low kurtosis (<3) suggests light tails or lack of outliers, which is case for 5/14 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5509d-8d1d-4d54-95ac-3cc7406613f7",
   "metadata": {},
   "source": [
    "# Normality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f33497-0f65-4628-83bf-317022b98117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the results\n",
    "columns = []\n",
    "shapiro_statistics = []\n",
    "shapiro_p_values = []\n",
    "shapiro_diagnosis = []\n",
    "ks_statistics = []\n",
    "ks_p_values = []\n",
    "ks_diagnosis = []\n",
    "\n",
    "# Define a function to determine the diagnosis based on the p-value\n",
    "def normality_diagnosis(p_value, alpha=0.05):\n",
    "    return 'Normal' if p_value > alpha else 'Not normal'\n",
    "\n",
    "# Testing normality for each numerical column\n",
    "for col in num.columns:\n",
    "    data = num[col].dropna()  # Exclude missing values from the test\n",
    "\n",
    "    # Shapiro-Wilk test with sample size check\n",
    "    if len(data) > 5000:\n",
    "        data_shapiro = data.sample(5000, random_state=1)\n",
    "    else:\n",
    "        data_shapiro = data\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(data_shapiro)\n",
    "    shapiro_statistics.append(shapiro_stat)\n",
    "    shapiro_p_values.append(shapiro_p)\n",
    "    shapiro_diagnosis.append(normality_diagnosis(shapiro_p))\n",
    "    \n",
    "    # Kolmogorov-Smirnov test with standardization\n",
    "    data_ks = (data - data.mean()) / data.std(ddof=0)\n",
    "    ks_stat, ks_p = stats.kstest(data_ks, 'norm')\n",
    "    ks_statistics.append(ks_stat)\n",
    "    ks_p_values.append(ks_p)\n",
    "    ks_diagnosis.append(normality_diagnosis(ks_p))\n",
    "    \n",
    "    columns.append(col)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "normality_test_df = pd.DataFrame({\n",
    "    'Column': columns,\n",
    "    'Shapiro Statistic': shapiro_statistics,\n",
    "    'Shapiro P-value': shapiro_p_values,\n",
    "    'Shapiro Diagnosis': shapiro_diagnosis,\n",
    "    'KS Statistic': ks_statistics,\n",
    "    'KS P-value': ks_p_values,\n",
    "    'KS Diagnosis': ks_diagnosis\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "normality_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2edc29-0b5d-4281-bf2a-cda2ab9a8c3b",
   "metadata": {},
   "source": [
    "- The numerical variables have been the subject to two normality test, starting with the null hypothesis:\n",
    "  - H0: The sample is derived from a population that follows a normal distribution.\n",
    "  - at the significance level (alpha) is set at 0.05.\n",
    "\n",
    " \n",
    "- All the p-values from the **Shapiro-Wilk** test are very low, indicating a rejection of the null hypothesis of normality for every variable.\n",
    "- Likewise, the **Kolmogorov-Smirnov** test results also show p-values of close to zero, leading to the same conclusion.\\\n",
    "With these consistent results from multiple normality tests, along with the high assymetry and and long tails, the evidence strongly suggests that the **data is not normally distributed**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
